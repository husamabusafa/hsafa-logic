---
title: Distributed Agent Platform Architecture
description: Building an Agent Builder + Distributed Agent Runtime (gateway) that scales into an agentic network (SmartSpaces + Entities) with persistent memory and distributed tool execution.
---

# üß† The Big Idea

You are building an **Agent Builder + Distributed Agent Runtime** that can grow into an **agentic system network**.

A system where:

* Users **create agents** via config
* Agents run **on your server** (the gateway is the runtime)
* Any client (web, mobile, node app) can:
  * Start a run
  * Watch it stream
  * Refresh and reconnect
* Agents can call **tools that run anywhere** (server, browser, other devices)

This is like giving AI agents a **backend, memory, and a network of devices**.

The update: instead of thinking only in ‚Äúone agent ‚Üî one chat‚Äù, the platform‚Äôs native model becomes **many Entities collaborating inside shared contexts** (SmartSpaces).

---

# üï∏Ô∏è Agentic Network Model (SmartSpaces + Entities)

## Core primitives

### SmartSpace

A **SmartSpace** is a shared context space (a timeline of events/messages) where participants collaborate.

- A SmartSpace can contain any number of participants.
- SmartSpaces can be public (many participants) or private (small, scoped contexts like ‚ÄúManager SmartSpace‚Äù).

### Entity

An **Entity** is a participant in the system. There are exactly two types:

- **Human Entity** (a user from your app)
- **Agent Entity** (an AI agent)

External services (Jira, Slack, IoT devices, payment gateways, cron jobs, Node.js backends) are **NOT entities**. They are **services** ‚Äî they trigger agents via API (`POST /api/agents/{agentId}/trigger`) and submit tool results, but have no space membership or identity in the system.

An Agent Entity can belong to multiple SmartSpaces.

### Client

A **Client** is a connection point where streams are delivered and responses are received.

- Examples: web browser, mobile app, Node.js backend, IoT device
- A Client is **not** an Entity ‚Äî it is a delivery channel

One Entity can connect from **multiple Clients** simultaneously. For example, a single Human Entity (user) might be logged in on both web and mobile at the same time. Both Clients receive the same stream; both can send messages as that Entity.

### Run (Agent execution)

A **Run** is a general-purpose execution of **one Agent Entity**. Runs are **not tied to any space** ‚Äî the agent interacts with spaces through tools (`sendSpaceMessage`, `readSpaceMessages`).

- A human message in a space triggers a run for the **admin agent** of that space
- An agent message with a `mention` triggers a run for the mentioned agent
- External services trigger runs directly via API (`POST /api/agents/{agentId}/trigger`)
- Scheduled plans trigger runs automatically
- The agent's LLM text output is internal (reasoning/planning). All visible communication happens through `sendSpaceMessage`.

### Step (inside a Run)

Inside a Run, execution is a multi-step reasoning-and-acting loop:

- **Run** = a complete multi-step AI interaction
- **Step** = a single LLM call within that Run

This distinction matters because the gateway streams **partial outputs** (text deltas, tool calls, tool results) while the Run is still in progress.

## Triggers (what starts a Run)

There are exactly three trigger types:

* **`space_message`** ‚Äî A human (or agent via `mention`) posted a message in a space
* **`plan`** ‚Äî The agent's own scheduled plan triggered it
* **`service`** ‚Äî An external service triggered the agent directly via API (`POST /api/agents/{agentId}/trigger`)

## Scheduling and long-term behavior

Each Agent Entity maintains:

* **Plan**: schedules future executions (cron-like). Example: ‚Äúsend a reminder at 6 PM‚Äù.
* **Goals**: short-term and long-term objectives that influence decisions over time.

The key design choice: **Plans and Goals are first-class state**, not just prompt text.

---

<details>
<summary>Architecture (later)</summary>

# üèóÔ∏è System Architecture

## 1Ô∏è‚É£ Agent Builder (Control Plane)

Users send agent configs:

```json
{
  "version": "1.0",
  "agent": {
    "name": "research-agent",
    "system": "..."
  },
  "model": {
    "provider": "openai",
    "name": "gpt-4.1-mini"
  },
  "tools": []
}
```

You:

* Store config in **durable storage**
* Reuse config when runs start

---

## 2Ô∏è‚É£ Agent Runtime (Execution Plane)

When a user starts a run:

1. Load agent config
2. Load chat history
3. Start the agent engine (the reasoning + tool loop)
4. Stream output
5. Handle tool calls
6. Save everything

Your system is the **body + memory + network**.

In the SmartSpace model, the gateway runtime expands slightly:

1. A message/event is written to a SmartSpace timeline.
2. The gateway triggers Runs for Agent Entities that belong to that SmartSpace.
3. Each Run streams events (text/tool-calls/tool-results) and may append new messages/events back into the SmartSpace.

---

</details>

# üîÅ Example scenario: Leave Request (Cross-SmartSpace)

A Human Entity asks an Agent Entity (in the Employee SmartSpace) to send a leave request to their manager.

1. The Human posts a message in the **Employee SmartSpace**.
2. The **admin agent** of the space is triggered with a general-purpose run.
3. The Agent calls `sendSpaceMessage(managerSpace, "Sarah is requesting 2 days PTO next week. Approve?", wait: { for: [{ type: "human" }] })` ‚Äî the message streams into the Manager SmartSpace and the tool blocks until the manager replies.
4. The Manager sees the message and replies "Approved."
5. The Agent's `wait` resolves with the manager's reply.
6. The Agent calls `sendSpaceMessage(employeeSpace, "Your leave request was approved!")` ‚Äî streams back to the employee.

All of this happens in **one general-purpose run**. No child runs, no goToSpace. The agent uses `sendSpaceMessage` with `wait` for cross-space request-response.

While waiting, if the employee asks for status, a **new run** is created. The agent can call `getMyRuns` to see it already has a run waiting on the manager's reply, and respond accordingly.

---

# üß∞ Display Tools and Space Routing

## `displayTool` + Auto-Injected `targetSpaceId`

Tool calls are internal by default. A tool becomes routable to SmartSpaces only when it is configured with:

```json
{
  "name": "searchDatabase",
  "displayTool": true
}
```

When `displayTool: true`, the gateway auto-injects optional `targetSpaceId` into the tool schema seen by the model.

- If the AI provides `targetSpaceId`, the gateway streams and persists that tool call in the target SmartSpace as a `tool_call` part.
- If the AI omits `targetSpaceId`, the tool executes normally and remains internal to the run stream.
- The gateway strips `targetSpaceId` before calling `execute()`.

### Routing Behavior

| `displayTool` | `targetSpaceId` in call | Result |
|---|---|---|
| `false` / omitted | any | Internal execution only (no space message) |
| `true` | omitted | Internal execution only (silent tool call) |
| `true` | set | Tool call appears in that SmartSpace's composite message |

### Internal / Prebuilt Tools

Prebuilt tools like goals, memory, planning, and context lookups are internal by design unless explicitly modeled as display tools.

---

# üé® Tool UI Configuration (Client-Side)

> **Note:** The gateway decides **whether** a tool call is visible in a SmartSpace (`displayTool` + `targetSpaceId`). The client SDK decides **how** that routed tool call is rendered.

## Architecture

**Gateway routing responsibilities:**
- Inject `targetSpaceId` only for tools with `displayTool: true`
- Relay routed tool events (`tool-input-start`, `tool-input-delta`, `tool-output-available`) to the target SmartSpace
- Persist routed tool calls as `tool_call` parts in composite messages

**Client rendering responsibilities:**
- Render routed tool parts in the timeline
- Attach custom UI components by tool name
- Fall back to default tool card UI when no custom component is registered

## Tool UI Config Example

```json
{
  "name": "searchDatabase",
  "displayTool": true,
  "display": {
    "customUI": "SearchResultsCard"
  }
}
```

- `displayTool` controls routing capability (`targetSpaceId` injection)
- `targetSpaceId` in each call controls where (or whether) the tool appears
- `display.customUI` controls the component used for rendering

---

# üß© Custom Tool UI Components (TODO)

> **Note:** This section describes planned functionality that has not been implemented yet.

Tools can have custom UI components that replace the default tool display. This enables rich, interactive experiences.

## Architecture

The Hsafa stack uses:
- **`@hsafa/react`** ‚Äî Hooks-only SDK for data fetching (`useMessages`, `useHsafaClient`, etc.)
- **`@assistant-ui/react`** ‚Äî UI primitives (`MessagePrimitive`, `ThreadPrimitive`, etc.)

Custom tool UIs are registered via `MessagePrimitive.Content` components.

## Registering Custom Tool UIs

### Option 1: Tools() API with Toolkit (Recommended)

The recommended approach is to define tools with their `render` function in a toolkit:

```tsx
import { useAui, Tools, type Toolkit } from "@assistant-ui/react";

const toolkit: Toolkit = {
  webSearch: {
    description: "Search the web",
    parameters: z.object({ query: z.string() }),
    // No execute - tool runs on gateway
    render: ({ args, result, status }) => (
      <SearchResultsCard
        query={args.query}
        results={result?.results}
        loading={status.type === "running"}
      />
    ),
  },
  
  requestApproval: {
    description: "Request user approval",
    parameters: z.object({ title: z.string(), amount: z.number() }),
    // No execute - basic tool, response via addResult
    render: ({ args, result, status, addResult }) => {
      if (result) return <div>{result.approved ? "‚úÖ Approved" : "‚ùå Rejected"}</div>;
      return (
        <div className="rounded border p-4">
          <p>{args.title}: ${args.amount}</p>
          <button onClick={() => addResult({ approved: true })}>Approve</button>
          <button onClick={() => addResult({ approved: false })}>Reject</button>
        </div>
      );
    },
  },
};

// Register in runtime provider
function MyRuntimeProvider({ children }) {
  const runtime = useHsafaRuntime({ /* ... */ });

  const aui = useAui({
    tools: Tools({ toolkit }),
  });

  return (
    <AssistantRuntimeProvider aui={aui} runtime={runtime}>
      {children}
    </AssistantRuntimeProvider>
  );
}
```

### Option 2: Per-Tool Components via MessagePrimitive.Content

For simpler setups or when using `useExternalStoreRuntime`, pass tool components directly:

```tsx
import { MessagePrimitive } from "@assistant-ui/react";

// Custom UI components for specific tools
const toolComponents = {
  webSearch: WebSearchToolUI,
  requestApproval: ApprovalToolUI,
};

function AssistantMessage() {
  return (
    <MessagePrimitive.Root>
      <MessagePrimitive.Content
        components={{
          tools: {
            ...toolComponents,
            Fallback: DefaultToolUI, // fallback for unregistered tools
          },
        }}
      />
    </MessagePrimitive.Root>
  );
}
```

This is what the `nextjs-test-app` currently uses with `useExternalStoreRuntime`.

## Tool UI Component Props

All tool UI components receive:

```tsx
interface ToolUIProps {
  toolName: string;
  toolCallId: string;
  args: Record<string, unknown>;
  argsText: string;                    // JSON stringified args
  result?: unknown;
  status: {
    type: "running" | "complete" | "incomplete" | "requires-action";
    reason?: string;                   // e.g., "error", "cancelled"
  };
  
  // For interactive tools - submit a result back to the gateway
  addResult?: (result: unknown) => void;
}
```

## Custom UI for Basic Tools

For **basic tools** (see [basic-tool.md](/hsafa-tools/basic-tool.md)), custom UIs can return a tool response via `addResult()`:

```tsx
function ApprovalToolUI({ args, result, status, addResult }: ToolUIProps) {
  // Already completed
  if (result) {
    return <div>{result.approved ? "‚úÖ Approved" : "‚ùå Rejected"}</div>;
  }

  // Waiting for user input
  return (
    <div className="rounded border p-4">
      <p>{args.title}</p>
      <div className="flex gap-2 mt-3">
        <button onClick={() => addResult({ approved: true })}>Approve</button>
        <button onClick={() => addResult({ approved: false })}>Reject</button>
      </div>
    </div>
  );
}
```

## Custom UI for Gateway-Executed Tools

Tools that execute on the gateway can still have custom visualization:

```tsx
function DatabaseQueryToolUI({ args, result, status }: ToolUIProps) {
  if (status.type === "running") {
    return <QueryLoadingState query={args.sql} />;
  }
  
  if (status.type === "incomplete" && status.reason === "error") {
    return <ErrorDisplay message="Query failed" />;
  }

  return <DataTable rows={result.rows} columns={result.columns} />;
}
```

## `displayTool` + Custom UI Integration

`displayTool` and `display.customUI` are complementary:

```json
{
  "name": "webSearch",
  "displayTool": true,
  "display": { "customUI": "WebSearchCard" }
}
```

- `displayTool: true` enables optional `targetSpaceId` routing
- A call with `targetSpaceId` renders a tool part in that SmartSpace
- A call without `targetSpaceId` stays internal (no SmartSpace tool part)
- `display.customUI` selects the renderer when the part is shown

## Fallback Behavior

If no custom UI is registered for a tool:
1. Render routed `tool_call` parts using the default `ToolFallback` component (collapsible with args/result)
2. Keep internal-only tool calls out of the SmartSpace timeline

```tsx
// Default fallback shows tool name, status, and expandable args/result
function DefaultToolUI({ toolName, args, status }: ToolUIProps) {
  const isRunning = status.type === "running";
  return (
    <div className="flex items-center gap-2 rounded border px-2 py-1 text-xs">
      {isRunning ? <LoaderIcon className="animate-spin" /> : <CheckIcon />}
      <span>{isRunning ? "Running" : "Completed"} {toolName}</span>
    </div>
  );
}
```

---

# üß∞ Prebuilt tools every Agent Entity should have

These are internal tools that Entities do not see or interact with:

1. **Plan management tools**

   * Update its schedule (Plan)
   * Read its scheduled actions

2. **Goal management tools**

   * Update its Goals
   * Read its Goals

3. **Memory tools**

   * Store long-term facts, user preferences, learnings
   * Retrieve memories by topic or Entity
   * Memory persists across Runs and SmartSpaces

4. **SmartSpace awareness tools**

   * Read the list of SmartSpaces it belongs to
   * Read messages/events from a specific SmartSpace

5. **Entity awareness tools**

   * List Entities in a SmartSpace
   * Look up Entity info (who is this person, what are their preferences)

6. **Space communication tools**

   * `sendSpaceMessage` ‚Äî send a message to any space (with optional `mention` to trigger another agent, optional `wait` to block for a reply)
   * `readSpaceMessages` ‚Äî read recent messages from any space
   * `delegateToAgent` ‚Äî (admin only) silently hand off to another agent
   * `getMyRuns` ‚Äî see own concurrent active runs

7. **SmartSpace management tools**

   * Create SmartSpaces
   * Delete SmartSpaces

8. **Time awareness tools**

   * Get current time
   * Understand how long ago events happened
   * Reason about deadlines and urgency

9. **Self-reflection tools**

   * Read its own past Runs
   * Summarize what it learned or did wrong
   * Track patterns in its behavior
